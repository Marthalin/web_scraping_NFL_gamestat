{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scrape for NFL Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://static01.nyt.com/images/2018/09/02/magazine/02MagNFL-1/02MagNFL-1-facebookJumbo-v4.jpg\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team Statistics from 2014 - 2019 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "import lxml.html as lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check url from week 1 to week 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.pro-football-reference.com/years/2014/week_1.htm'\n",
    "\n",
    "result_array = []\n",
    "for j in range(5):\n",
    "    url = 'https://www.pro-football-reference.com/years/' + str(2014+j) + '/week_1.htm'\n",
    "    print(url)\n",
    "    \n",
    "    for i in range(17):\n",
    "        url = 'https://www.pro-football-reference.com/years/' + str(2014+j) + '/week_' + str(1+i) + '.htm'\n",
    "        page = urlopen(url)\n",
    "        soup = BeautifulSoup(page, 'lxml')\n",
    "        all_matches = soup.findAll(attrs = {'class':['right gamelink']})\n",
    "\n",
    "        for each in all_matches:\n",
    "\n",
    "            for link in each.findAll('a'):        \n",
    "                gamedata_url = \"https://www.pro-football-reference.com\" + link.attrs['href']\n",
    "                print(gamedata_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract passing, rushing and receiving tables from the website for 5 seasons (2014-2018) respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl = 'https://www.pro-football-reference.com/years/2014/week_1.htm'\n",
    "\n",
    "result_array_2014 = []\n",
    "for i in range(17):\n",
    "    url = 'https://www.pro-football-reference.com/years/2014/week_' + str(1+i) + '.htm'\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    all_matches = soup.findAll(attrs = {'class':['right gamelink']})\n",
    "\n",
    "    for each in all_matches:\n",
    "\n",
    "        for link in each.findAll('a'):        \n",
    "            gamedata_url = \"https://www.pro-football-reference.com\" + link.attrs['href']\n",
    "\n",
    "            stat_page = requests.get(gamedata_url)\n",
    "            stat_doc = lh.fromstring(stat_page.content)\n",
    "            tr_elements = stat_doc.xpath(\"//tr\")\n",
    "            game_id = []\n",
    "            \n",
    "            \n",
    "            col = []\n",
    "            i = 0\n",
    "\n",
    "            for j in range(0, len(tr_elements)):\n",
    "                T = tr_elements[j]\n",
    "\n",
    "                if len(T) == 22:\n",
    "    #                print(j)\n",
    "                    for t in tr_elements[j]:\n",
    "                        i += 1\n",
    "                        name = str(i)+t.text_content()\n",
    "    #                    print('%d : \"%s\"' %(i, name))\n",
    "                        col.append((name, []))\n",
    "\n",
    "                    j!=0\n",
    "                    break\n",
    "\n",
    "\n",
    "            for x in range(j+1, len(tr_elements)):\n",
    "                T = tr_elements[x]\n",
    "\n",
    "                if len(T)!=22:\n",
    "                    break\n",
    "\n",
    "                i = 0\n",
    "\n",
    "                for t in T.iterchildren():\n",
    "                    data = t.text_content()\n",
    "\n",
    "                    if i > 0:\n",
    "                        try:\n",
    "                            data = int(data)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    col[i][1].append(data)\n",
    "                    i += 1\n",
    "\n",
    "                Dict = {title:column for (title, column) in col}\n",
    "                df = pd.DataFrame(Dict)\n",
    "\n",
    "    #get 2nd team's data by adding 3 rows\n",
    "                for y in range(x+2, len(tr_elements)):\n",
    "                    T2 = tr_elements[y]\n",
    "\n",
    "                    if len(T2)!=22:\n",
    "                           break\n",
    "\n",
    "                    i = 0            \n",
    "                    for t in T2.iterchildren():\n",
    "                        data = t.text_content()\n",
    "\n",
    "                        if i > 0:\n",
    "                            try:\n",
    "                                data = int(data)\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "                        col[i][1].append(data)\n",
    "                        i += 1\n",
    "\n",
    "                    Dict = {title:column for (title, column) in col}\n",
    "                    df = pd.DataFrame(Dict)\n",
    "\n",
    "    #add game id for each row\n",
    "            v = link.attrs['href']\n",
    "            for a in range(0, len(Dict['1Player'])):\n",
    "                game_id.append(v)\n",
    "            df['game_id'] = game_id\n",
    "            \n",
    "            df.drop_duplicates(subset =\"1Player\", keep = 'first', inplace = True)\n",
    "    \n",
    "            result_array_2014.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.pro-football-reference.com/years/2015/week_1.htm'\n",
    "\n",
    "result_array_2015 = []\n",
    "for i in range(17):\n",
    "    url = 'https://www.pro-football-reference.com/years/2015/week_' + str(1+i) + '.htm'\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    all_matches = soup.findAll(attrs = {'class':['right gamelink']})\n",
    "\n",
    "    for each in all_matches:\n",
    "\n",
    "        for link in each.findAll('a'):        \n",
    "            gamedata_url = \"https://www.pro-football-reference.com\" + link.attrs['href']\n",
    "\n",
    "            stat_page = requests.get(gamedata_url)\n",
    "            stat_doc = lh.fromstring(stat_page.content)\n",
    "            tr_elements = stat_doc.xpath(\"//tr\")\n",
    "            game_id = []\n",
    "            \n",
    "            \n",
    "            col = []\n",
    "            i = 0\n",
    "\n",
    "            for j in range(0, len(tr_elements)):\n",
    "                T = tr_elements[j]\n",
    "\n",
    "                if len(T) == 22:\n",
    "    #                print(j)\n",
    "                    for t in tr_elements[j]:\n",
    "                        i += 1\n",
    "                        name = str(i)+t.text_content()\n",
    "    #                    print('%d : \"%s\"' %(i, name))\n",
    "                        col.append((name, []))\n",
    "\n",
    "                    j!=0\n",
    "                    break\n",
    "\n",
    "\n",
    "            for x in range(j+1, len(tr_elements)):\n",
    "                T = tr_elements[x]\n",
    "\n",
    "                if len(T)!=22:\n",
    "                    break\n",
    "\n",
    "                i = 0\n",
    "\n",
    "                for t in T.iterchildren():\n",
    "                    data = t.text_content()\n",
    "\n",
    "                    if i > 0:\n",
    "                        try:\n",
    "                            data = int(data)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    col[i][1].append(data)\n",
    "                    i += 1\n",
    "\n",
    "                Dict = {title:column for (title, column) in col}\n",
    "                df = pd.DataFrame(Dict)\n",
    "\n",
    "    #get 2nd team's data by adding 2 rows\n",
    "                for y in range(x+2, len(tr_elements)):\n",
    "                    T2 = tr_elements[y]\n",
    "\n",
    "                    if len(T2)!=22:\n",
    "                           break\n",
    "\n",
    "                    i = 0            \n",
    "                    for t in T2.iterchildren():\n",
    "                        data = t.text_content()\n",
    "\n",
    "                        if i > 0:\n",
    "                            try:\n",
    "                                data = int(data)\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "                        col[i][1].append(data)\n",
    "                        i += 1\n",
    "\n",
    "                    Dict = {title:column for (title, column) in col}\n",
    "                    df = pd.DataFrame(Dict)\n",
    "\n",
    "    #add game id for each row\n",
    "            v = link.attrs['href']\n",
    "            for a in range(0, len(Dict['1Player'])):\n",
    "                game_id.append(v)\n",
    "            df['game_id'] = game_id\n",
    "            \n",
    "            df.drop_duplicates(subset =\"1Player\", keep = 'first', inplace = True)\n",
    "    \n",
    "            result_array_2015.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.pro-football-reference.com/years/2016/week_1.htm'\n",
    "\n",
    "result_array_2016 = []\n",
    "for i in range(17):\n",
    "    url = 'https://www.pro-football-reference.com/years/2016/week_' + str(1+i) + '.htm'\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    all_matches = soup.findAll(attrs = {'class':['right gamelink']})\n",
    "\n",
    "    for each in all_matches:\n",
    "\n",
    "        for link in each.findAll('a'):        \n",
    "            gamedata_url = \"https://www.pro-football-reference.com\" + link.attrs['href']\n",
    "\n",
    "            stat_page = requests.get(gamedata_url)\n",
    "            stat_doc = lh.fromstring(stat_page.content)\n",
    "            tr_elements = stat_doc.xpath(\"//tr\")\n",
    "            game_id = []\n",
    "            \n",
    "            \n",
    "            col = []\n",
    "            i = 0\n",
    "\n",
    "            for j in range(0, len(tr_elements)):\n",
    "                T = tr_elements[j]\n",
    "\n",
    "                if len(T) == 22:\n",
    "    #                print(j)\n",
    "                    for t in tr_elements[j]:\n",
    "                        i += 1\n",
    "                        name = str(i)+t.text_content()\n",
    "    #                    print('%d : \"%s\"' %(i, name))\n",
    "                        col.append((name, []))\n",
    "\n",
    "                    j!=0\n",
    "                    break\n",
    "\n",
    "\n",
    "            for x in range(j+1, len(tr_elements)):\n",
    "                T = tr_elements[x]\n",
    "\n",
    "                if len(T)!=22:\n",
    "                    break\n",
    "\n",
    "                i = 0\n",
    "\n",
    "                for t in T.iterchildren():\n",
    "                    data = t.text_content()\n",
    "\n",
    "                    if i > 0:\n",
    "                        try:\n",
    "                            data = int(data)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    col[i][1].append(data)\n",
    "                    i += 1\n",
    "\n",
    "                Dict = {title:column for (title, column) in col}\n",
    "                df = pd.DataFrame(Dict)\n",
    "\n",
    "    #get 2nd team's data by adding 2 rows\n",
    "                for y in range(x+2, len(tr_elements)):\n",
    "                    T2 = tr_elements[y]\n",
    "\n",
    "                    if len(T2)!=22:\n",
    "                           break\n",
    "\n",
    "                    i = 0            \n",
    "                    for t in T2.iterchildren():\n",
    "                        data = t.text_content()\n",
    "\n",
    "                        if i > 0:\n",
    "                            try:\n",
    "                                data = int(data)\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "                        col[i][1].append(data)\n",
    "                        i += 1\n",
    "\n",
    "                    Dict = {title:column for (title, column) in col}\n",
    "                    df = pd.DataFrame(Dict)\n",
    "\n",
    "    #add game id for each row\n",
    "            v = link.attrs['href']\n",
    "            for a in range(0, len(Dict['1Player'])):\n",
    "                game_id.append(v)\n",
    "            df['game_id'] = game_id\n",
    "            \n",
    "            df.drop_duplicates(subset =\"1Player\", keep = 'first', inplace = True)\n",
    "    \n",
    "            result_array_2016.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.pro-football-reference.com/years/2017/week_1.htm'\n",
    "\n",
    "result_array_2017 = []\n",
    "for i in range(17):\n",
    "    url = 'https://www.pro-football-reference.com/years/2017/week_' + str(1+i) + '.htm'\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    all_matches = soup.findAll(attrs = {'class':['right gamelink']})\n",
    "\n",
    "    for each in all_matches:\n",
    "\n",
    "        for link in each.findAll('a'):        \n",
    "            gamedata_url = \"https://www.pro-football-reference.com\" + link.attrs['href']\n",
    "\n",
    "            stat_page = requests.get(gamedata_url)\n",
    "            stat_doc = lh.fromstring(stat_page.content)\n",
    "            tr_elements = stat_doc.xpath(\"//tr\")\n",
    "            game_id = []\n",
    "            \n",
    "            \n",
    "            col = []\n",
    "            i = 0\n",
    "\n",
    "            for j in range(0, len(tr_elements)):\n",
    "                T = tr_elements[j]\n",
    "\n",
    "                if len(T) == 22:\n",
    "    #                print(j)\n",
    "                    for t in tr_elements[j]:\n",
    "                        i += 1\n",
    "                        name = str(i)+t.text_content()\n",
    "    #                    print('%d : \"%s\"' %(i, name))\n",
    "                        col.append((name, []))\n",
    "\n",
    "                    j!=0\n",
    "                    break\n",
    "\n",
    "\n",
    "            for x in range(j+1, len(tr_elements)):\n",
    "                T = tr_elements[x]\n",
    "\n",
    "                if len(T)!=22:\n",
    "                    break\n",
    "\n",
    "                i = 0\n",
    "\n",
    "                for t in T.iterchildren():\n",
    "                    data = t.text_content()\n",
    "\n",
    "                    if i > 0:\n",
    "                        try:\n",
    "                            data = int(data)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    col[i][1].append(data)\n",
    "                    i += 1\n",
    "\n",
    "                Dict = {title:column for (title, column) in col}\n",
    "                df = pd.DataFrame(Dict)\n",
    "\n",
    "    #get 2nd team's data by adding 2 rows\n",
    "                for y in range(x+2, len(tr_elements)):\n",
    "                    T2 = tr_elements[y]\n",
    "\n",
    "                    if len(T2)!=22:\n",
    "                           break\n",
    "\n",
    "                    i = 0            \n",
    "                    for t in T2.iterchildren():\n",
    "                        data = t.text_content()\n",
    "\n",
    "                        if i > 0:\n",
    "                            try:\n",
    "                                data = int(data)\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "                        col[i][1].append(data)\n",
    "                        i += 1\n",
    "\n",
    "                    Dict = {title:column for (title, column) in col}\n",
    "                    df = pd.DataFrame(Dict)\n",
    "\n",
    "    #add game id for each row\n",
    "            v = link.attrs['href']\n",
    "            for a in range(0, len(Dict['1Player'])):\n",
    "                game_id.append(v)\n",
    "            df['game_id'] = game_id\n",
    "            \n",
    "            df.drop_duplicates(subset =\"1Player\", keep = 'first', inplace = True)\n",
    "    \n",
    "            result_array_2017.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.pro-football-reference.com/years/2018/week_1.htm'\n",
    "\n",
    "result_array_2018 = []\n",
    "for i in range(17):\n",
    "    url = 'https://www.pro-football-reference.com/years/2018/week_' + str(1+i) + '.htm'\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    all_matches = soup.findAll(attrs = {'class':['right gamelink']})\n",
    "\n",
    "    for each in all_matches:\n",
    "\n",
    "        for link in each.findAll('a'):        \n",
    "            gamedata_url = \"https://www.pro-football-reference.com\" + link.attrs['href']\n",
    "\n",
    "            stat_page = requests.get(gamedata_url)\n",
    "            stat_doc = lh.fromstring(stat_page.content)\n",
    "            tr_elements = stat_doc.xpath(\"//tr\")\n",
    "            game_id = []\n",
    "            \n",
    "            \n",
    "            col = []\n",
    "            i = 0\n",
    "\n",
    "            for j in range(0, len(tr_elements)):\n",
    "                T = tr_elements[j]\n",
    "\n",
    "                if len(T) == 22:\n",
    "    #                print(j)\n",
    "                    for t in tr_elements[j]:\n",
    "                        i += 1\n",
    "                        name = str(i)+t.text_content()\n",
    "    #                    print('%d : \"%s\"' %(i, name))\n",
    "                        col.append((name, []))\n",
    "\n",
    "                    j!=0\n",
    "                    break\n",
    "\n",
    "\n",
    "            for x in range(j+1, len(tr_elements)):\n",
    "                T = tr_elements[x]\n",
    "\n",
    "                if len(T)!=22:\n",
    "                    break\n",
    "\n",
    "                i = 0\n",
    "\n",
    "                for t in T.iterchildren():\n",
    "                    data = t.text_content()\n",
    "\n",
    "                    if i > 0:\n",
    "                        try:\n",
    "                            data = int(data)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    col[i][1].append(data)\n",
    "                    i += 1\n",
    "\n",
    "                Dict = {title:column for (title, column) in col}\n",
    "                df = pd.DataFrame(Dict)\n",
    "\n",
    "    #get 2nd team's data by adding 2 rows\n",
    "                for y in range(x+2, len(tr_elements)):\n",
    "                    T2 = tr_elements[y]\n",
    "\n",
    "                    if len(T2)!=22:\n",
    "                           break\n",
    "\n",
    "                    i = 0            \n",
    "                    for t in T2.iterchildren():\n",
    "                        data = t.text_content()\n",
    "\n",
    "                        if i > 0:\n",
    "                            try:\n",
    "                                data = int(data)\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "                        col[i][1].append(data)\n",
    "                        i += 1\n",
    "\n",
    "                    Dict = {title:column for (title, column) in col}\n",
    "                    df = pd.DataFrame(Dict)\n",
    "\n",
    "    #add game id for each row\n",
    "            v = link.attrs['href']\n",
    "            for a in range(0, len(Dict['1Player'])):\n",
    "                game_id.append(v)\n",
    "            df['game_id'] = game_id\n",
    "            \n",
    "            df.drop_duplicates(subset =\"1Player\", keep = 'first', inplace = True)\n",
    "    \n",
    "            result_array_2018.append(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract passing, rushing and receiving tables from the website for 2019 week1 - week 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.pro-football-reference.com/years/2019/week_1.htm'\n",
    "\n",
    "result_array_2019 = []\n",
    "for i in range(13):\n",
    "    url = 'https://www.pro-football-reference.com/years/2019/week_' + str(1+i) + '.htm'\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    all_matches = soup.findAll(attrs = {'class':['right gamelink']})\n",
    "\n",
    "    for each in all_matches:\n",
    "\n",
    "        for link in each.findAll('a'):        \n",
    "            gamedata_url = \"https://www.pro-football-reference.com\" + link.attrs['href']\n",
    "\n",
    "            stat_page = requests.get(gamedata_url)\n",
    "            stat_doc = lh.fromstring(stat_page.content)\n",
    "            tr_elements = stat_doc.xpath(\"//tr\")\n",
    "            game_id = []\n",
    "            \n",
    "            \n",
    "            col = []\n",
    "            i = 0\n",
    "\n",
    "            for j in range(0, len(tr_elements)):\n",
    "                T = tr_elements[j]\n",
    "\n",
    "                if len(T) == 22:\n",
    "    #                print(j)\n",
    "                    for t in tr_elements[j]:\n",
    "                        i += 1\n",
    "                        name = str(i)+t.text_content()\n",
    "    #                    print('%d : \"%s\"' %(i, name))\n",
    "                        col.append((name, []))\n",
    "\n",
    "                    j!=0\n",
    "                    break\n",
    "\n",
    "\n",
    "            for x in range(j+1, len(tr_elements)):\n",
    "                T = tr_elements[x]\n",
    "\n",
    "                if len(T)!=22:\n",
    "                    break\n",
    "\n",
    "                i = 0\n",
    "\n",
    "                for t in T.iterchildren():\n",
    "                    data = t.text_content()\n",
    "\n",
    "                    if i > 0:\n",
    "                        try:\n",
    "                            data = int(data)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    col[i][1].append(data)\n",
    "                    i += 1\n",
    "\n",
    "                Dict = {title:column for (title, column) in col}\n",
    "                df = pd.DataFrame(Dict)\n",
    "\n",
    "    #get 2nd team's data by adding 3 rows\n",
    "                for y in range(x+2, len(tr_elements)):\n",
    "                    T2 = tr_elements[y]\n",
    "\n",
    "                    if len(T2)!=22:\n",
    "                           break\n",
    "\n",
    "                    i = 0            \n",
    "                    for t in T2.iterchildren():\n",
    "                        data = t.text_content()\n",
    "\n",
    "                        if i > 0:\n",
    "                            try:\n",
    "                                data = int(data)\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "                        col[i][1].append(data)\n",
    "                        i += 1\n",
    "\n",
    "                    Dict = {title:column for (title, column) in col}\n",
    "                    df = pd.DataFrame(Dict)\n",
    "\n",
    "    #add game id for each row\n",
    "            v = link.attrs['href']\n",
    "            for a in range(0, len(Dict['1Player'])):\n",
    "                game_id.append(v)\n",
    "            df['game_id'] = game_id\n",
    "            \n",
    "            df.drop_duplicates(subset =\"1Player\", keep = 'first', inplace = True)\n",
    "    \n",
    "            result_array_2019.append(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concatenate all the data and save as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_array = result_array_2014 + result_array_2015 + result_array_2016 + result_array_2017 + result_array_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_team_stat = pd.concat(result_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_team_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_team_stat_2019 = pd.concat(result_array_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_team_stat.to_csv('nfl_team_stat.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_team_stat_2019.to_csv('nfl_team_stat_2019.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoreboard of Each Game from 2014 - 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scoreboard without OT for 5 seasons (2014-2018) respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.pro-football-reference.com/years/2014/week_1.htm'\n",
    "\n",
    "score_2014 = []\n",
    "\n",
    "for i in range(17):\n",
    "    url = 'https://www.pro-football-reference.com/years/2014/week_' + str(1+i) + '.htm'\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    all_matches = soup.findAll(attrs = {'class':['right gamelink']})\n",
    "\n",
    "    for each in all_matches:\n",
    "\n",
    "        for link in each.findAll('a'):        \n",
    "            gamedata_url = \"https://www.pro-football-reference.com\" + link.attrs['href']\n",
    "\n",
    "            stat_page = requests.get(gamedata_url)\n",
    "            stat_doc = lh.fromstring(stat_page.content)\n",
    "            tr_elements = stat_doc.xpath(\"//tr\")\n",
    "            game_id = []\n",
    "\n",
    "            score = []\n",
    "            i = 0\n",
    "\n",
    "            for t in tr_elements[0]:\n",
    "                i += 1\n",
    "                name = t.text_content()\n",
    "#                print('%d : \"%s\"' %(i, name))\n",
    "                score.append((name, []))\n",
    "\n",
    "            for j in range(1, len(tr_elements)):\n",
    "                T = tr_elements[j]\n",
    "\n",
    "                if (len(T) != 7) :\n",
    "                    break\n",
    "\n",
    "                i = 0\n",
    "\n",
    "                for t in T.iterchildren():\n",
    "                    data = t.text_content()\n",
    "\n",
    "                    if i > 0:\n",
    "                        try:\n",
    "                            data = int(data)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    score[i][1].append(data)\n",
    "                    i += 1\n",
    "\n",
    "            Dict = {title:column for (title, column) in score}\n",
    "            df_score = pd.DataFrame(Dict)\n",
    "\n",
    "            v = link.attrs['href']\n",
    "            for a in range(0, len(Dict['Final'])):\n",
    "                game_id.append(v)\n",
    "            df_score['game_id'] = game_id\n",
    "            \n",
    "        score_2014.append(df_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.pro-football-reference.com/years/2015/week_1.htm'\n",
    "\n",
    "score_2015 = []\n",
    "\n",
    "for i in range(17):\n",
    "    url = 'https://www.pro-football-reference.com/years/2015/week_' + str(1+i) + '.htm'\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    all_matches = soup.findAll(attrs = {'class':['right gamelink']})\n",
    "\n",
    "    for each in all_matches:\n",
    "\n",
    "        for link in each.findAll('a'):        \n",
    "            gamedata_url = \"https://www.pro-football-reference.com\" + link.attrs['href']\n",
    "\n",
    "            stat_page = requests.get(gamedata_url)\n",
    "            stat_doc = lh.fromstring(stat_page.content)\n",
    "            tr_elements = stat_doc.xpath(\"//tr\")\n",
    "            game_id = []\n",
    "\n",
    "            score = []\n",
    "            i = 0\n",
    "\n",
    "            for t in tr_elements[0]:\n",
    "                i += 1\n",
    "                name = t.text_content()\n",
    "#                print('%d : \"%s\"' %(i, name))\n",
    "                score.append((name, []))\n",
    "\n",
    "            for j in range(1, len(tr_elements)):\n",
    "                T = tr_elements[j]\n",
    "\n",
    "                if (len(T) != 7) :\n",
    "                    break\n",
    "\n",
    "                i = 0\n",
    "\n",
    "                for t in T.iterchildren():\n",
    "                    data = t.text_content()\n",
    "\n",
    "                    if i > 0:\n",
    "                        try:\n",
    "                            data = int(data)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    score[i][1].append(data)\n",
    "                    i += 1\n",
    "\n",
    "            Dict = {title:column for (title, column) in score}\n",
    "            df_score = pd.DataFrame(Dict)\n",
    "\n",
    "            v = link.attrs['href']\n",
    "            for a in range(0, len(Dict['Final'])):\n",
    "                game_id.append(v)\n",
    "            df_score['game_id'] = game_id\n",
    "            \n",
    "        score_2015.append(df_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.pro-football-reference.com/years/2016/week_1.htm'\n",
    "\n",
    "score_2016 = []\n",
    "\n",
    "for i in range(17):\n",
    "    url = 'https://www.pro-football-reference.com/years/2016/week_' + str(1+i) + '.htm'\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    all_matches = soup.findAll(attrs = {'class':['right gamelink']})\n",
    "\n",
    "    for each in all_matches:\n",
    "\n",
    "        for link in each.findAll('a'):        \n",
    "            gamedata_url = \"https://www.pro-football-reference.com\" + link.attrs['href']\n",
    "\n",
    "            stat_page = requests.get(gamedata_url)\n",
    "            stat_doc = lh.fromstring(stat_page.content)\n",
    "            tr_elements = stat_doc.xpath(\"//tr\")\n",
    "            game_id = []\n",
    "\n",
    "            score = []\n",
    "            i = 0\n",
    "\n",
    "            for t in tr_elements[0]:\n",
    "                i += 1\n",
    "                name = t.text_content()\n",
    "#                print('%d : \"%s\"' %(i, name))\n",
    "                score.append((name, []))\n",
    "\n",
    "            for j in range(1, len(tr_elements)):\n",
    "                T = tr_elements[j]\n",
    "\n",
    "                if (len(T) != 7) :\n",
    "                    break\n",
    "\n",
    "                i = 0\n",
    "\n",
    "                for t in T.iterchildren():\n",
    "                    data = t.text_content()\n",
    "\n",
    "                    if i > 0:\n",
    "                        try:\n",
    "                            data = int(data)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    score[i][1].append(data)\n",
    "                    i += 1\n",
    "\n",
    "            Dict = {title:column for (title, column) in score}\n",
    "            df_score = pd.DataFrame(Dict)\n",
    "\n",
    "            v = link.attrs['href']\n",
    "            for a in range(0, len(Dict['Final'])):\n",
    "                game_id.append(v)\n",
    "            df_score['game_id'] = game_id\n",
    "            \n",
    "        score_2016.append(df_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.pro-football-reference.com/years/2017/week_1.htm'\n",
    "\n",
    "score_2017 = []\n",
    "\n",
    "for i in range(17):\n",
    "    url = 'https://www.pro-football-reference.com/years/2017/week_' + str(1+i) + '.htm'\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    all_matches = soup.findAll(attrs = {'class':['right gamelink']})\n",
    "\n",
    "    for each in all_matches:\n",
    "\n",
    "        for link in each.findAll('a'):        \n",
    "            gamedata_url = \"https://www.pro-football-reference.com\" + link.attrs['href']\n",
    "\n",
    "            stat_page = requests.get(gamedata_url)\n",
    "            stat_doc = lh.fromstring(stat_page.content)\n",
    "            tr_elements = stat_doc.xpath(\"//tr\")\n",
    "            game_id = []\n",
    "\n",
    "            score = []\n",
    "            i = 0\n",
    "\n",
    "            for t in tr_elements[0]:\n",
    "                i += 1\n",
    "                name = t.text_content()\n",
    "#                print('%d : \"%s\"' %(i, name))\n",
    "                score.append((name, []))\n",
    "\n",
    "            for j in range(1, len(tr_elements)):\n",
    "                T = tr_elements[j]\n",
    "\n",
    "                if (len(T) != 7) :\n",
    "                    break\n",
    "\n",
    "                i = 0\n",
    "\n",
    "                for t in T.iterchildren():\n",
    "                    data = t.text_content()\n",
    "\n",
    "                    if i > 0:\n",
    "                        try:\n",
    "                            data = int(data)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    score[i][1].append(data)\n",
    "                    i += 1\n",
    "\n",
    "            Dict = {title:column for (title, column) in score}\n",
    "            df_score = pd.DataFrame(Dict)\n",
    "\n",
    "            v = link.attrs['href']\n",
    "            for a in range(0, len(Dict['Final'])):\n",
    "                game_id.append(v)\n",
    "            df_score['game_id'] = game_id\n",
    "            \n",
    "        score_2017.append(df_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.pro-football-reference.com/years/2018/week_1.htm'\n",
    "\n",
    "score_2018 = []\n",
    "\n",
    "for i in range(17):\n",
    "    url = 'https://www.pro-football-reference.com/years/2018/week_' + str(1+i) + '.htm'\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    all_matches = soup.findAll(attrs = {'class':['right gamelink']})\n",
    "\n",
    "    for each in all_matches:\n",
    "\n",
    "        for link in each.findAll('a'):        \n",
    "            gamedata_url = \"https://www.pro-football-reference.com\" + link.attrs['href']\n",
    "\n",
    "            stat_page = requests.get(gamedata_url)\n",
    "            stat_doc = lh.fromstring(stat_page.content)\n",
    "            tr_elements = stat_doc.xpath(\"//tr\")\n",
    "            game_id = []\n",
    "\n",
    "            score = []\n",
    "            i = 0\n",
    "\n",
    "            for t in tr_elements[0]:\n",
    "                i += 1\n",
    "                name = t.text_content()\n",
    "#                print('%d : \"%s\"' %(i, name))\n",
    "                score.append((name, []))\n",
    "\n",
    "            for j in range(1, len(tr_elements)):\n",
    "                T = tr_elements[j]\n",
    "\n",
    "                if (len(T) != 7) :\n",
    "                    break\n",
    "\n",
    "                i = 0\n",
    "\n",
    "                for t in T.iterchildren():\n",
    "                    data = t.text_content()\n",
    "\n",
    "                    if i > 0:\n",
    "                        try:\n",
    "                            data = int(data)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    score[i][1].append(data)\n",
    "                    i += 1\n",
    "\n",
    "            Dict = {title:column for (title, column) in score}\n",
    "            df_score = pd.DataFrame(Dict)\n",
    "\n",
    "            v = link.attrs['href']\n",
    "            for a in range(0, len(Dict['Final'])):\n",
    "                game_id.append(v)\n",
    "            df_score['game_id'] = game_id\n",
    "            \n",
    "        score_2018.append(df_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scoreboard with OT for 5 seasons (2014-2018) respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.pro-football-reference.com/years/2014/week_1.htm'\n",
    "\n",
    "score_ot_2014 = []\n",
    "\n",
    "for i in range(17):\n",
    "    url = 'https://www.pro-football-reference.com/years/2014/week_' + str(1+i) + '.htm'\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    all_matches = soup.findAll(attrs = {'class':['right gamelink']})\n",
    "\n",
    "    for each in all_matches:\n",
    "\n",
    "        for link in each.findAll('a'):        \n",
    "            gamedata_url = \"https://www.pro-football-reference.com\" + link.attrs['href']\n",
    "\n",
    "            stat_page = requests.get(gamedata_url)\n",
    "            stat_doc = lh.fromstring(stat_page.content)\n",
    "            tr_elements = stat_doc.xpath(\"//tr\")\n",
    "            game_id = []\n",
    "\n",
    "            score = []\n",
    "            i = 0\n",
    "\n",
    "            for t in tr_elements[0]:\n",
    "                i += 1\n",
    "                name = t.text_content()\n",
    "#                print('%d : \"%s\"' %(i, name))\n",
    "                score.append((name, []))\n",
    "\n",
    "            for j in range(1, len(tr_elements)):\n",
    "                T = tr_elements[j]\n",
    "\n",
    "                if (len(T) != 8) :\n",
    "                    break\n",
    "\n",
    "                i = 0\n",
    "\n",
    "                for t in T.iterchildren():\n",
    "                    data = t.text_content()\n",
    "\n",
    "                    if i > 0:\n",
    "                        try:\n",
    "                            data = int(data)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    score[i][1].append(data)\n",
    "                    i += 1\n",
    "\n",
    "            Dict = {title:column for (title, column) in score}\n",
    "            df_score = pd.DataFrame(Dict)\n",
    "\n",
    "            v = link.attrs['href']\n",
    "            for a in range(0, len(Dict['Final'])):\n",
    "                game_id.append(v)\n",
    "            df_score['game_id'] = game_id\n",
    "            \n",
    "        score_ot_2014.append(df_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.pro-football-reference.com/years/2015/week_1.htm'\n",
    "\n",
    "score_ot_2015 = []\n",
    "\n",
    "for i in range(17):\n",
    "    url = 'https://www.pro-football-reference.com/years/2015/week_' + str(1+i) + '.htm'\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    all_matches = soup.findAll(attrs = {'class':['right gamelink']})\n",
    "\n",
    "    for each in all_matches:\n",
    "\n",
    "        for link in each.findAll('a'):        \n",
    "            gamedata_url = \"https://www.pro-football-reference.com\" + link.attrs['href']\n",
    "\n",
    "            stat_page = requests.get(gamedata_url)\n",
    "            stat_doc = lh.fromstring(stat_page.content)\n",
    "            tr_elements = stat_doc.xpath(\"//tr\")\n",
    "            game_id = []\n",
    "\n",
    "            score = []\n",
    "            i = 0\n",
    "\n",
    "            for t in tr_elements[0]:\n",
    "                i += 1\n",
    "                name = t.text_content()\n",
    "#                print('%d : \"%s\"' %(i, name))\n",
    "                score.append((name, []))\n",
    "\n",
    "            for j in range(1, len(tr_elements)):\n",
    "                T = tr_elements[j]\n",
    "\n",
    "                if (len(T) != 8) :\n",
    "                    break\n",
    "\n",
    "                i = 0\n",
    "\n",
    "                for t in T.iterchildren():\n",
    "                    data = t.text_content()\n",
    "\n",
    "                    if i > 0:\n",
    "                        try:\n",
    "                            data = int(data)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    score[i][1].append(data)\n",
    "                    i += 1\n",
    "\n",
    "            Dict = {title:column for (title, column) in score}\n",
    "            df_score = pd.DataFrame(Dict)\n",
    "\n",
    "            v = link.attrs['href']\n",
    "            for a in range(0, len(Dict['Final'])):\n",
    "                game_id.append(v)\n",
    "            df_score['game_id'] = game_id\n",
    "            \n",
    "        score_ot_2015.append(df_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.pro-football-reference.com/years/2016/week_1.htm'\n",
    "\n",
    "score_ot_2016 = []\n",
    "\n",
    "for i in range(17):\n",
    "    url = 'https://www.pro-football-reference.com/years/2016/week_' + str(1+i) + '.htm'\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    all_matches = soup.findAll(attrs = {'class':['right gamelink']})\n",
    "\n",
    "    for each in all_matches:\n",
    "\n",
    "        for link in each.findAll('a'):        \n",
    "            gamedata_url = \"https://www.pro-football-reference.com\" + link.attrs['href']\n",
    "\n",
    "            stat_page = requests.get(gamedata_url)\n",
    "            stat_doc = lh.fromstring(stat_page.content)\n",
    "            tr_elements = stat_doc.xpath(\"//tr\")\n",
    "            game_id = []\n",
    "\n",
    "            score = []\n",
    "            i = 0\n",
    "\n",
    "            for t in tr_elements[0]:\n",
    "                i += 1\n",
    "                name = t.text_content()\n",
    "#                print('%d : \"%s\"' %(i, name))\n",
    "                score.append((name, []))\n",
    "\n",
    "            for j in range(1, len(tr_elements)):\n",
    "                T = tr_elements[j]\n",
    "\n",
    "                if (len(T) != 8) :\n",
    "                    break\n",
    "\n",
    "                i = 0\n",
    "\n",
    "                for t in T.iterchildren():\n",
    "                    data = t.text_content()\n",
    "\n",
    "                    if i > 0:\n",
    "                        try:\n",
    "                            data = int(data)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    score[i][1].append(data)\n",
    "                    i += 1\n",
    "\n",
    "            Dict = {title:column for (title, column) in score}\n",
    "            df_score = pd.DataFrame(Dict)\n",
    "\n",
    "            v = link.attrs['href']\n",
    "            for a in range(0, len(Dict['Final'])):\n",
    "                game_id.append(v)\n",
    "            df_score['game_id'] = game_id\n",
    "            \n",
    "        score_ot_2016.append(df_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.pro-football-reference.com/years/2017/week_1.htm'\n",
    "\n",
    "score_ot_2017 = []\n",
    "\n",
    "for i in range(17):\n",
    "    url = 'https://www.pro-football-reference.com/years/2017/week_' + str(1+i) + '.htm'\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    all_matches = soup.findAll(attrs = {'class':['right gamelink']})\n",
    "\n",
    "    for each in all_matches:\n",
    "\n",
    "        for link in each.findAll('a'):        \n",
    "            gamedata_url = \"https://www.pro-football-reference.com\" + link.attrs['href']\n",
    "\n",
    "            stat_page = requests.get(gamedata_url)\n",
    "            stat_doc = lh.fromstring(stat_page.content)\n",
    "            tr_elements = stat_doc.xpath(\"//tr\")\n",
    "            game_id = []\n",
    "\n",
    "            score = []\n",
    "            i = 0\n",
    "\n",
    "            for t in tr_elements[0]:\n",
    "                i += 1\n",
    "                name = t.text_content()\n",
    "#                print('%d : \"%s\"' %(i, name))\n",
    "                score.append((name, []))\n",
    "\n",
    "            for j in range(1, len(tr_elements)):\n",
    "                T = tr_elements[j]\n",
    "\n",
    "                if (len(T) != 8) :\n",
    "                    break\n",
    "\n",
    "                i = 0\n",
    "\n",
    "                for t in T.iterchildren():\n",
    "                    data = t.text_content()\n",
    "\n",
    "                    if i > 0:\n",
    "                        try:\n",
    "                            data = int(data)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    score[i][1].append(data)\n",
    "                    i += 1\n",
    "\n",
    "            Dict = {title:column for (title, column) in score}\n",
    "            df_score = pd.DataFrame(Dict)\n",
    "\n",
    "            v = link.attrs['href']\n",
    "            for a in range(0, len(Dict['Final'])):\n",
    "                game_id.append(v)\n",
    "            df_score['game_id'] = game_id\n",
    "            \n",
    "        score_ot_2017.append(df_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.pro-football-reference.com/years/2018/week_1.htm'\n",
    "\n",
    "score_ot_2018 = []\n",
    "\n",
    "for i in range(17):\n",
    "    url = 'https://www.pro-football-reference.com/years/2018/week_' + str(1+i) + '.htm'\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    all_matches = soup.findAll(attrs = {'class':['right gamelink']})\n",
    "\n",
    "    for each in all_matches:\n",
    "\n",
    "        for link in each.findAll('a'):        \n",
    "            gamedata_url = \"https://www.pro-football-reference.com\" + link.attrs['href']\n",
    "\n",
    "            stat_page = requests.get(gamedata_url)\n",
    "            stat_doc = lh.fromstring(stat_page.content)\n",
    "            tr_elements = stat_doc.xpath(\"//tr\")\n",
    "            game_id = []\n",
    "\n",
    "            score = []\n",
    "            i = 0\n",
    "\n",
    "            for t in tr_elements[0]:\n",
    "                i += 1\n",
    "                name = t.text_content()\n",
    "#                print('%d : \"%s\"' %(i, name))\n",
    "                score.append((name, []))\n",
    "\n",
    "            for j in range(1, len(tr_elements)):\n",
    "                T = tr_elements[j]\n",
    "\n",
    "                if (len(T) != 8) :\n",
    "                    break\n",
    "\n",
    "                i = 0\n",
    "\n",
    "                for t in T.iterchildren():\n",
    "                    data = t.text_content()\n",
    "\n",
    "                    if i > 0:\n",
    "                        try:\n",
    "                            data = int(data)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    score[i][1].append(data)\n",
    "                    i += 1\n",
    "\n",
    "            Dict = {title:column for (title, column) in score}\n",
    "            df_score = pd.DataFrame(Dict)\n",
    "\n",
    "            v = link.attrs['href']\n",
    "            for a in range(0, len(Dict['Final'])):\n",
    "                game_id.append(v)\n",
    "            df_score['game_id'] = game_id\n",
    "            \n",
    "        score_ot_2018.append(df_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scoreboard without OT for 2019 week 1 - week 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.pro-football-reference.com/years/2019/week_1.htm'\n",
    "\n",
    "score_2019 = []\n",
    "\n",
    "for i in range(13):\n",
    "    url = 'https://www.pro-football-reference.com/years/2019/week_' + str(1+i) + '.htm'\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    all_matches = soup.findAll(attrs = {'class':['right gamelink']})\n",
    "\n",
    "    for each in all_matches:\n",
    "\n",
    "        for link in each.findAll('a'):        \n",
    "            gamedata_url = \"https://www.pro-football-reference.com\" + link.attrs['href']\n",
    "\n",
    "            stat_page = requests.get(gamedata_url)\n",
    "            stat_doc = lh.fromstring(stat_page.content)\n",
    "            tr_elements = stat_doc.xpath(\"//tr\")\n",
    "            game_id = []\n",
    "\n",
    "            score = []\n",
    "            i = 0\n",
    "\n",
    "            for t in tr_elements[0]:\n",
    "                i += 1\n",
    "                name = t.text_content()\n",
    "#                print('%d : \"%s\"' %(i, name))\n",
    "                score.append((name, []))\n",
    "\n",
    "            for j in range(1, len(tr_elements)):\n",
    "                T = tr_elements[j]\n",
    "\n",
    "                if (len(T) != 7) :\n",
    "                    break\n",
    "\n",
    "                i = 0\n",
    "\n",
    "                for t in T.iterchildren():\n",
    "                    data = t.text_content()\n",
    "\n",
    "                    if i > 0:\n",
    "                        try:\n",
    "                            data = int(data)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    score[i][1].append(data)\n",
    "                    i += 1\n",
    "\n",
    "            Dict = {title:column for (title, column) in score}\n",
    "            df_score = pd.DataFrame(Dict)\n",
    "\n",
    "            v = link.attrs['href']\n",
    "            for a in range(0, len(Dict['Final'])):\n",
    "                game_id.append(v)\n",
    "            df_score['game_id'] = game_id\n",
    "            \n",
    "        score_2019.append(df_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scoreboard with OT for 2019 week 1 - week 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.pro-football-reference.com/years/2019/week_1.htm'\n",
    "\n",
    "score_ot_2019 = []\n",
    "\n",
    "for i in range(13):\n",
    "    url = 'https://www.pro-football-reference.com/years/2019/week_' + str(1+i) + '.htm'\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    all_matches = soup.findAll(attrs = {'class':['right gamelink']})\n",
    "\n",
    "    for each in all_matches:\n",
    "\n",
    "        for link in each.findAll('a'):        \n",
    "            gamedata_url = \"https://www.pro-football-reference.com\" + link.attrs['href']\n",
    "\n",
    "            stat_page = requests.get(gamedata_url)\n",
    "            stat_doc = lh.fromstring(stat_page.content)\n",
    "            tr_elements = stat_doc.xpath(\"//tr\")\n",
    "            game_id = []\n",
    "\n",
    "            score = []\n",
    "            i = 0\n",
    "\n",
    "            for t in tr_elements[0]:\n",
    "                i += 1\n",
    "                name = t.text_content()\n",
    "#                print('%d : \"%s\"' %(i, name))\n",
    "                score.append((name, []))\n",
    "\n",
    "            for j in range(1, len(tr_elements)):\n",
    "                T = tr_elements[j]\n",
    "\n",
    "                if (len(T) != 8) :\n",
    "                    break\n",
    "\n",
    "                i = 0\n",
    "\n",
    "                for t in T.iterchildren():\n",
    "                    data = t.text_content()\n",
    "\n",
    "                    if i > 0:\n",
    "                        try:\n",
    "                            data = int(data)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    score[i][1].append(data)\n",
    "                    i += 1\n",
    "\n",
    "            Dict = {title:column for (title, column) in score}\n",
    "            df_score = pd.DataFrame(Dict)\n",
    "\n",
    "            v = link.attrs['href']\n",
    "            for a in range(0, len(Dict['Final'])):\n",
    "                game_id.append(v)\n",
    "            df_score['game_id'] = game_id\n",
    "            \n",
    "        score_ot_2019.append(df_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concatenate all the data and save as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_array = score_2014 + score_ot_2014 + score_2015 + score_ot_2015 + score_2016 + score_ot_2016 + score_2017 + score_ot_2017 + score_2018 + score_ot_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_game_score = pd.concat(score_array, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_game_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_array_2019 = score_2019 + score_ot_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_game_score_2019 = pd.concat(score_array_2019, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_game_score.to_csv('nfl_game_score.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_game_score_2019.to_csv('nfl_game_score_2019.csv', sep = ',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
